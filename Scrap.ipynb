{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826f5d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice's Adventures in Wonderland Lewis Carroll 30609 downloads to books/Alice's Adventures in Wonderland Lewis Carroll 30609 downloads.txt\n",
      "Anne of Green Gables L. M. Montgomery 9175 downloads to books/Anne of Green Gables L. M. Montgomery 9175 downloads.txt\n",
      "Treasure Island Robert Louis Stevenson 9038 downloads to books/Treasure Island Robert Louis Stevenson 9038 downloads.txt\n",
      "A Christmas Carol in Prose; Being a Ghost Story of Christmas Charles Dickens 8467 downloads to books/A Christmas Carol in Prose; Being a Ghost Story of Christmas Charles Dickens 8467 downloads.txt\n",
      "The Wonderful Wizard of Oz L. Frank Baum 8360 downloads to books/The Wonderful Wizard of Oz L. Frank Baum 8360 downloads.txt\n",
      "Peter Pan J. M. Barrie 7701 downloads to books/Peter Pan J. M. Barrie 7701 downloads.txt\n",
      "Little Women Louisa May Alcott 7685 downloads to books/Little Women Louisa May Alcott 7685 downloads.txt\n",
      "The Jungle Book Rudyard Kipling 5394 downloads to books/The Jungle Book Rudyard Kipling 5394 downloads.txt\n",
      "The Legend of Sleepy Hollow Washington Irving 5381 downloads to books/The Legend of Sleepy Hollow Washington Irving 5381 downloads.txt\n",
      "The Secret Garden Frances Hodgson Burnett 4465 downloads to books/The Secret Garden Frances Hodgson Burnett 4465 downloads.txt\n",
      "Through the Looking-Glass Lewis Carroll 4377 downloads to books/Through the Looking-Glass Lewis Carroll 4377 downloads.txt\n",
      "The Happy Prince, and Other Tales Oscar Wilde 4370 downloads to books/The Happy Prince, and Other Tales Oscar Wilde 4370 downloads.txt\n",
      "Just So Stories Rudyard Kipling 2762 downloads to books/Just So Stories Rudyard Kipling 2762 downloads.txt\n",
      "The Wind in the Willows Kenneth Grahame 2737 downloads to books/The Wind in the Willows Kenneth Grahame 2737 downloads.txt\n",
      "The Railway Children E. Nesbit 2113 downloads to books/The Railway Children E. Nesbit 2113 downloads.txt\n",
      "The Secret Garden Frances Hodgson Burnett 2110 downloads to books/The Secret Garden Frances Hodgson Burnett 2110 downloads.txt\n",
      "Alice's Adventures in Wonderland Lewis Carroll 2075 downloads to books/Alice's Adventures in Wonderland Lewis Carroll 2075 downloads.txt\n",
      "Kidnapped Robert Louis Stevenson 2066 downloads to books/Kidnapped Robert Louis Stevenson 2066 downloads.txt\n",
      "Black Beauty Anna Sewell 2058 downloads to books/Black Beauty Anna Sewell 2058 downloads.txt\n",
      "Anne of Avonlea L. M. Montgomery 1986 downloads to books/Anne of Avonlea L. M. Montgomery 1986 downloads.txt\n",
      "A Child's Garden of Verses Robert Louis Stevenson 1925 downloads to books/A Child's Garden of Verses Robert Louis Stevenson 1925 downloads.txt\n",
      "Old Granny Fox Thornton W. Burgess 1854 downloads to books/Old Granny Fox Thornton W. Burgess 1854 downloads.txt\n",
      "The Blue Fairy Book Andrew Lang 1830 downloads to books/The Blue Fairy Book Andrew Lang 1830 downloads.txt\n",
      "Beautiful Stories from Shakespeare William Shakespeare and E. Nesbit 1806 downloads to books/Beautiful Stories from Shakespeare William Shakespeare and E. Nesbit 1806 downloads.txt\n",
      "Anne of the Island L. M. Montgomery 1755 downloads to books/Anne of the Island L. M. Montgomery 1755 downloads.txt\n",
      "A Little Princess Frances Hodgson Burnett 1737 downloads to books/A Little Princess Frances Hodgson Burnett 1737 downloads.txt\n",
      "The Princess and the Goblin George MacDonald 1625 downloads to books/The Princess and the Goblin George MacDonald 1625 downloads.txt\n",
      "The Velveteen Rabbit Margery Williams Bianco 1552 downloads to books/The Velveteen Rabbit Margery Williams Bianco 1552 downloads.txt\n",
      "A Christmas Carol Charles Dickens 1485 downloads to books/A Christmas Carol Charles Dickens 1485 downloads.txt\n",
      "Anne's House of Dreams L. M. Montgomery 1416 downloads to books/Anne's House of Dreams L. M. Montgomery 1416 downloads.txt\n",
      "Heidi Johanna Spyri 1129 downloads to books/Heidi Johanna Spyri 1129 downloads.txt\n",
      "Twice-told tales Nathaniel Hawthorne 1079 downloads to books/Twice-told tales Nathaniel Hawthorne 1079 downloads.txt\n",
      "The Arabian Nights: Their Best-known Tales 1032 downloads to books/The Arabian Nights: Their Best-known Tales 1032 downloads.txt\n",
      "The Story of Doctor Dolittle Hugh Lofting 1005 downloads to books/The Story of Doctor Dolittle Hugh Lofting 1005 downloads.txt\n",
      "Kim Rudyard Kipling 960 downloads to books/Kim Rudyard Kipling 960 downloads.txt\n",
      "A Christmas Carol Charles Dickens 958 downloads to books/A Christmas Carol Charles Dickens 958 downloads.txt\n",
      "The Marvelous Land of Oz L. Frank Baum 942 downloads to books/The Marvelous Land of Oz L. Frank Baum 942 downloads.txt\n",
      "Rilla of Ingleside L. M. Montgomery 888 downloads to books/Rilla of Ingleside L. M. Montgomery 888 downloads.txt\n",
      "Little Men: Life at Plumfield With Jo's Boys Louisa May Alcott 838 downloads to books/Little Men: Life at Plumfield With Jo's Boys Louisa May Alcott 838 downloads.txt\n",
      "Rainbow Valley L. M. Montgomery 709 downloads to books/Rainbow Valley L. M. Montgomery 709 downloads.txt\n",
      "The Book of Dragons E. Nesbit 671 downloads to books/The Book of Dragons E. Nesbit 671 downloads.txt\n",
      "Dorothy and the Wizard in Oz L. Frank Baum 666 downloads to books/Dorothy and the Wizard in Oz L. Frank Baum 666 downloads.txt\n",
      "Little Lord Fauntleroy Frances Hodgson Burnett 644 downloads to books/Little Lord Fauntleroy Frances Hodgson Burnett 644 downloads.txt\n",
      "Five Children and It E. Nesbit 615 downloads to books/Five Children and It E. Nesbit 615 downloads.txt\n",
      "The Red Fairy Book 599 downloads to books/The Red Fairy Book 599 downloads.txt\n",
      "Uncle Remus, His Songs and His Sayings Joel Chandler Harris 571 downloads to books/Uncle Remus, His Songs and His Sayings Joel Chandler Harris 571 downloads.txt\n",
      "Alice's Adventures Under Ground Lewis Carroll 534 downloads to books/Alice's Adventures Under Ground Lewis Carroll 534 downloads.txt\n",
      "The Story Girl L. M. Montgomery 499 downloads to books/The Story Girl L. M. Montgomery 499 downloads.txt\n",
      "The Voyages of Doctor Dolittle Hugh Lofting 491 downloads to books/The Voyages of Doctor Dolittle Hugh Lofting 491 downloads.txt\n",
      "American Fairy Tales L. Frank Baum 486 downloads to books/American Fairy Tales L. Frank Baum 486 downloads.txt\n",
      "The Song of Hiawatha Henry Wadsworth Longfellow 478 downloads to books/The Song of Hiawatha Henry Wadsworth Longfellow 478 downloads.txt\n",
      "Jo's Boys Louisa May Alcott 467 downloads to books/Jo's Boys Louisa May Alcott 467 downloads.txt\n",
      "Jack and Jill Louisa May Alcott 462 downloads to books/Jack and Jill Louisa May Alcott 462 downloads.txt\n",
      "The Story of the Treasure Seekers E. Nesbit 460 downloads to books/The Story of the Treasure Seekers E. Nesbit 460 downloads.txt\n",
      "The Water-Babies: A Fairy Tale for a Land-Baby Charles Kingsley 447 downloads to books/The Water-Babies: A Fairy Tale for a Land-Baby Charles Kingsley 447 downloads.txt\n",
      "Alice's Abenteuer im Wunderland (German) Lewis Carroll 435 downloads to books/Alice's Abenteuer im Wunderland (German) Lewis Carroll 435 downloads.txt\n",
      "The Second Jungle Book Rudyard Kipling 420 downloads to books/The Second Jungle Book Rudyard Kipling 420 downloads.txt\n",
      "Chronicles of Avonlea L. M. Montgomery 395 downloads to books/Chronicles of Avonlea L. M. Montgomery 395 downloads.txt\n",
      "The Hunting of the Snark: An Agony in Eight Fits Lewis Carroll 395 downloads to books/The Hunting of the Snark: An Agony in Eight Fits Lewis Carroll 395 downloads.txt\n",
      "Five Little Peppers and How They Grew Margaret Sidney 389 downloads to books/Five Little Peppers and How They Grew Margaret Sidney 389 downloads.txt\n",
      "An Old-Fashioned Girl Louisa May Alcott 388 downloads to books/An Old-Fashioned Girl Louisa May Alcott 388 downloads.txt\n",
      "Whitefoot the Wood Mouse Thornton W. Burgess 380 downloads to books/Whitefoot the Wood Mouse Thornton W. Burgess 380 downloads.txt\n",
      "At the Back of the North Wind George MacDonald 376 downloads to books/At the Back of the North Wind George MacDonald 376 downloads.txt\n",
      "The Violet Fairy Book Andrew Lang 368 downloads to books/The Violet Fairy Book Andrew Lang 368 downloads.txt\n",
      "Eight Cousins Louisa May Alcott 365 downloads to books/Eight Cousins Louisa May Alcott 365 downloads.txt\n",
      "The Yellow Fairy Book Andrew Lang 362 downloads to books/The Yellow Fairy Book Andrew Lang 362 downloads.txt\n",
      "The Golden Road L. M. Montgomery 359 downloads to books/The Golden Road L. M. Montgomery 359 downloads.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rose in Bloom Louisa May Alcott 347 downloads to books/Rose in Bloom Louisa May Alcott 347 downloads.txt\n",
      "The Pink Fairy Book Andrew Lang 343 downloads to books/The Pink Fairy Book Andrew Lang 343 downloads.txt\n",
      "The Light Princess George MacDonald 341 downloads to books/The Light Princess George MacDonald 341 downloads.txt\n",
      "The Phoenix and the Carpet E. Nesbit 319 downloads to books/The Phoenix and the Carpet E. Nesbit 319 downloads.txt\n",
      "Flower Fables Louisa May Alcott 315 downloads to books/Flower Fables Louisa May Alcott 315 downloads.txt\n",
      "The Adventures of Reddy Fox Thornton W. Burgess 302 downloads to books/The Adventures of Reddy Fox Thornton W. Burgess 302 downloads.txt\n",
      "Rebecca of Sunnybrook Farm Kate Douglas Smith Wiggin 297 downloads to books/Rebecca of Sunnybrook Farm Kate Douglas Smith Wiggin 297 downloads.txt\n",
      "Tanglewood Tales Nathaniel Hawthorne 290 downloads to books/Tanglewood Tales Nathaniel Hawthorne 290 downloads.txt\n",
      "Kilmeny of the Orchard L. M. Montgomery 288 downloads to books/Kilmeny of the Orchard L. M. Montgomery 288 downloads.txt\n",
      "Old Mother West Wind Thornton W. Burgess 286 downloads to books/Old Mother West Wind Thornton W. Burgess 286 downloads.txt\n",
      "Heidi Johanna Spyri 280 downloads to books/Heidi Johanna Spyri 280 downloads.txt\n",
      "A Modern Cinderella; Or, The Little Old Shoe, and Other Stories Louisa May Alcott 274 downloads to books/A Modern Cinderella; Or, The Little Old Shoe, and Other Stories Louisa May Alcott 274 downloads.txt\n",
      "The Lost Princess of Oz L. Frank Baum 266 downloads to books/The Lost Princess of Oz L. Frank Baum 266 downloads.txt\n",
      "What Katy Did Next Susan Coolidge 264 downloads to books/What Katy Did Next Susan Coolidge 264 downloads.txt\n",
      "The Story of the Amulet E. Nesbit 259 downloads to books/The Story of the Amulet E. Nesbit 259 downloads.txt\n",
      "A Child's Garden of Verses Robert Louis Stevenson 255 downloads to books/A Child's Garden of Verses Robert Louis Stevenson 255 downloads.txt\n",
      "Alice's Adventures in Wonderland Lewis Carroll 253 downloads to books/Alice's Adventures in Wonderland Lewis Carroll 253 downloads.txt\n",
      "Puck of Pook's Hill Rudyard Kipling 248 downloads to books/Puck of Pook's Hill Rudyard Kipling 248 downloads.txt\n",
      "Understood Betsy Dorothy Canfield Fisher 247 downloads to books/Understood Betsy Dorothy Canfield Fisher 247 downloads.txt\n",
      "Heidis Lehr- und Wanderjahre (German) Johanna Spyri 240 downloads to books/Heidis Lehr- und Wanderjahre (German) Johanna Spyri 240 downloads.txt\n",
      "A Dog of Flanders Ouida 233 downloads to books/A Dog of Flanders Ouida 233 downloads.txt\n",
      "The Green Fairy Book 232 downloads to books/The Green Fairy Book 232 downloads.txt\n",
      "The Magic of Oz L. Frank Baum 228 downloads to books/The Magic of Oz L. Frank Baum 228 downloads.txt\n",
      "The Crimson Fairy Book Andrew Lang 225 downloads to books/The Crimson Fairy Book Andrew Lang 225 downloads.txt\n",
      "The Cricket on the Hearth: A Fairy Tale of Home Charles Dickens 222 downloads to books/The Cricket on the Hearth: A Fairy Tale of Home Charles Dickens 222 downloads.txt\n",
      "Uncle Tom's Cabin, Young Folks' Edition Harriet Beecher Stowe 217 downloads to books/Uncle Tom's Cabin, Young Folks' Edition Harriet Beecher Stowe 217 downloads.txt\n",
      "What Katy Did Susan Coolidge 215 downloads to books/What Katy Did Susan Coolidge 215 downloads.txt\n",
      "The Magic City E. Nesbit 214 downloads to books/The Magic City E. Nesbit 214 downloads.txt\n",
      "The Lost Prince Frances Hodgson Burnett 214 downloads to books/The Lost Prince Frances Hodgson Burnett 214 downloads.txt\n",
      "Ozma of Oz L. Frank Baum 207 downloads to books/Ozma of Oz L. Frank Baum 207 downloads.txt\n",
      "The Princess and Curdie George MacDonald 204 downloads to books/The Princess and Curdie George MacDonald 204 downloads.txt\n",
      "The Master Key L. Frank Baum 196 downloads to books/The Master Key L. Frank Baum 196 downloads.txt\n",
      "Alice's Adventures in Wonderland Lewis Carroll 195 downloads to books/Alice's Adventures in Wonderland Lewis Carroll 195 downloads.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "\n",
    "page_urls = [\n",
    "    \"https://www.gutenberg.org/ebooks/bookshelf/20\", \n",
    "    \"https://www.gutenberg.org/ebooks/bookshelf/20?start_index=26\", \n",
    "    \"https://www.gutenberg.org/ebooks/bookshelf/20?start_index=51\", \n",
    "    \"https://www.gutenberg.org/ebooks/bookshelf/20?start_index=76\", \n",
    "]\n",
    "\n",
    "for url in page_urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    book_links = soup.select('li.booklink a')\n",
    "    os.makedirs(\"books\", exist_ok=True)\n",
    "    for link in book_links:\n",
    "        title = link.text.strip().replace('/', '-').replace('\\n', ' ').replace('\\r', '')\n",
    "        book_id = link['href'].split(\"/\")[-1]\n",
    "        book_url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-h/{book_id}-h.htm\"\n",
    "        book_response = requests.get(book_url)\n",
    "        book_content = book_response.content\n",
    "        book_soup = BeautifulSoup(book_content, 'html.parser')\n",
    "        book_text = book_soup.find('body').get_text()\n",
    "        filename = f\"books/{title}.txt\"\n",
    "        with open(filename, \"w\", encoding='utf-8') as file:\n",
    "            file.write(book_text)\n",
    "\n",
    "        print(f\"{title} to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1940c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "# spécifier le chemin du dossier contenant les fichiers\n",
    "folder_path = 'books'\n",
    "\n",
    "# boucler à travers tous les fichiers dans le dossier\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            # détecter l'encodage de caractères du fichier\n",
    "            result = chardet.detect(f.read())\n",
    "            encoding = result['encoding']\n",
    "            \n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "\n",
    "            # lire le contenu du fichier\n",
    "            text = f.read()\n",
    "            \n",
    "            # supprimer les URLs\n",
    "            text = re.sub(r'http\\S+', '', text)\n",
    "            \n",
    "            # supprimer la ponctuation\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            \n",
    "            # supprimer les nombres\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "            \n",
    "            # remplacer les espaces non entre les mots par une nouvelle ligne\n",
    "            text = re.sub(r'(?<!\\w)\\s+|\\s+(?!\\w)', '\\n', text)\n",
    "            \n",
    "            # supprimer les lignes vides ou qui ne contiennent pas de texte\n",
    "            text = '\\n'.join(line.strip() for line in text.split('\\n') if line.strip())\n",
    "            \n",
    "            # supprimer les mots \"pg\"\n",
    "            text = re.sub(r'\\b pg\\b', '', text)\n",
    "            \n",
    "            # supprimer les caractères spéciaux\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            \n",
    "            # supprimer les lignes contenant \"CONTENTS\" ou \"contents\" ou \"chapter\" + chiffre romain ou \"CHAPTER\" + chiffre romain\n",
    "            text = re.sub(r'^(CONTENTS|Contents|contents|chapter\\s+[IVXLCM]+|CHAPTER\\s+[IVXLCM]+Chapter\\s+[IVXLCM]+).*$\\n?', '', text, flags=re.MULTILINE)\n",
    "     \n",
    "        # écrire le texte traité dans le même fichier\n",
    "        with open(file_path, 'w', encoding=encoding) as f:\n",
    "            f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be27159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "# spécifier le chemin du dossier contenant les fichiers\n",
    "folder_path = 'books'\n",
    "\n",
    "# boucler à travers tous les fichiers dans le dossier\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            # détecter l'encodage de caractères du fichier\n",
    "            result = chardet.detect(f.read())\n",
    "            encoding = result['encoding']\n",
    "\n",
    "        # initialiser le compteur de lignes à 0\n",
    "        line_count = 0\n",
    "\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            # initialiser la variable pour stocker les lignes du fichier\n",
    "            lines = []\n",
    "            \n",
    "            for line in f:\n",
    "                \n",
    "                if line:\n",
    "                    line_count += 1\n",
    "                    \n",
    "                    if line_count > 100:\n",
    "                        # ajouter la ligne à la liste des lignes\n",
    "                        lines.append(line)\n",
    "                \n",
    "                    if line_count >= 200:\n",
    "                        # si on a atteint le nombre maximum de lignes, sortir de la boucle\n",
    "                        break\n",
    "\n",
    "            # concaténer les lignes en un seul texte\n",
    "            text = ''.join(lines)\n",
    "\n",
    "        # écrire le texte traité dans le même fichier\n",
    "        with open(file_path, 'w', encoding=encoding) as f:\n",
    "            f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "709f1de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "'''import csv\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "import os\n",
    "\n",
    "# Define the path to the directory where the books are stored\n",
    "books_dir = \"books\"\n",
    "\n",
    "# Define the path to the directory where the summaries will be stored\n",
    "summaries_dir = \"summaries\"\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "if not os.path.exists(books_dir):\n",
    "    os.makedirs(books_dir)\n",
    "\n",
    "if not os.path.exists(summaries_dir):\n",
    "    os.makedirs(summaries_dir)\n",
    "\n",
    "# Create the CSV file and write the header\n",
    "with open(\"book_summaries.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"text\", \"summary\"])\n",
    "\n",
    "    # Process each book in the directory\n",
    "    for filename in os.listdir(books_dir):\n",
    "        book_filepath = os.path.join(books_dir, filename)\n",
    "\n",
    "        # Read the book text from the file\n",
    "        with open(book_filepath, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Summarize the text using the TextRank algorithm\n",
    "        summarizer = TextRankSummarizer()\n",
    "        tokenizer = Tokenizer(\"english\")\n",
    "        parser = PlaintextParser(text, tokenizer)\n",
    "        summary = summarizer(parser.document, sentences_count=3)\n",
    "\n",
    "        # Write the summary to a file with the same name as the original file\n",
    "        summary_filename = os.path.splitext(filename)[0] + \"_summary.txt\"\n",
    "        summary_filepath = os.path.join(summaries_dir, summary_filename)\n",
    "        with open(summary_filepath, \"w\", encoding=\"utf-8\") as summary_file:\n",
    "            summary_file.write(str(summary))\n",
    "\n",
    "        # Write the book text and summary text to the CSV file\n",
    "        writer.writerow([text, str(summary)])\n",
    "\n",
    "print(\"Done.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "361cb07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "import os\n",
    "\n",
    "# Define the path to the directory where the books are stored\n",
    "books_dir = \"books\"\n",
    "\n",
    "# Define the path to the directory where the summaries will be stored\n",
    "summaries_dir = \"summaries\"\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "if not os.path.exists(books_dir):\n",
    "    os.makedirs(books_dir)\n",
    "\n",
    "if not os.path.exists(summaries_dir):\n",
    "    os.makedirs(summaries_dir)\n",
    "\n",
    "# Create the CSV file and write the header\n",
    "with open(\"book_summaries.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"text\", \"summary\"])\n",
    "\n",
    "    # Process each book in the directory\n",
    "    for filename in os.listdir(books_dir):\n",
    "        book_filepath = os.path.join(books_dir, filename)\n",
    "\n",
    "        # Read the book text from the file\n",
    "        with open(book_filepath, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Summarize the text using the TextRank algorithm\n",
    "        summarizer = TextRankSummarizer()\n",
    "        tokenizer = Tokenizer(\"english\")\n",
    "        parser = PlaintextParser(text, tokenizer)\n",
    "        summary = summarizer(parser.document, sentences_count=3)\n",
    "\n",
    "        # Write the summary to a file with the same name as the original file\n",
    "        summary_filename = os.path.splitext(filename)[0] + \"_summary.txt\"\n",
    "        summary_filepath = os.path.join(summaries_dir, summary_filename)\n",
    "        with open(summary_filepath, \"w\", encoding=\"utf-8\") as summary_file:\n",
    "            summary_file.write(str(summary))\n",
    "\n",
    "        # Write the book text and summary text to the CSV file\n",
    "        writer.writerow([text.replace('\\n', ' '), str(summary).replace('\\n', ' ')])\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bcdcdff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# création de la matrice TF-IDF\u001b[39;00m\n\u001b[0;32m     38\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 39\u001b[0m tf_idf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# calcul du score de pertinence des phrases\u001b[39;00m\n\u001b[0;32m     42\u001b[0m sentence_scores \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2131\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2126\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2127\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2128\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2129\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2130\u001b[0m )\n\u001b[1;32m-> 2131\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1293\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# chemin du dossier contenant les fichiers texte\n",
    "directory = \"summaries/\"\n",
    "\n",
    "# chemin du dossier où les fichiers résumés seront stockés\n",
    "summary_directory = \"summaries/summaries/\"\n",
    "\n",
    "# création du répertoire de stockage s'il n'existe pas\n",
    "if not os.path.exists(summary_directory):\n",
    "    os.makedirs(summary_directory)\n",
    "\n",
    "# initialisation de la liste des stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# boucle sur les fichiers dans le dossier \"summaries\"\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # chemin complet du fichier texte\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # lecture du contenu du fichier\n",
    "        with open(filepath, \"r\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # tokenization des phrases et des mots\n",
    "        sentences = sent_tokenize(text)\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # suppression de la ponctuation et des stopwords\n",
    "        words = [word.lower() for word in words if word not in string.punctuation and word.lower() not in stop_words]\n",
    "        \n",
    "        # création de la matrice TF-IDF\n",
    "        vectorizer = TfidfVectorizer(norm=None)\n",
    "        tf_idf_matrix = vectorizer.fit_transform(sentences)\n",
    "        \n",
    "        # calcul du score de pertinence des phrases\n",
    "        sentence_scores = {}\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            feature_scores = tf_idf_matrix.getcol(i).toarray()\n",
    "            sentence_scores[i] = sum(feature_scores[j] for j in range(len(feature_scores)))\n",
    "        \n",
    "        # sélection des phrases les plus pertinentes pour le résumé\n",
    "        summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:3]\n",
    "        summary_sentences = sorted(summary_sentences)\n",
    "        summary = ' '.join([sentences[i] for i in summary_sentences])\n",
    "        \n",
    "        # écriture du résumé dans un nouveau fichier\n",
    "        summary_filename = os.path.splitext(filename)[0] + \"_summary.txt\"\n",
    "        summary_filepath = os.path.join(summary_directory, summary_filename)\n",
    "        with open(summary_filepath, \"w\") as f:\n",
    "            f.write(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3599fec6",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'summaries\\\\summaries'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m book_filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(books_dir, filename)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Read the book text from the file\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbook_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mISO-8859-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     31\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Summarize the text using the TextRank algorithm\u001b[39;00m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'summaries\\\\summaries'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "import os\n",
    "\n",
    "# Define the path to the directory where the books are stored\n",
    "books_dir = \"summaries\"\n",
    "\n",
    "# Define the path to the directory where the summaries will be stored\n",
    "summaries_dir = \"summariess\"\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "if not os.path.exists(books_dir):\n",
    "    os.makedirs(books_dir)\n",
    "\n",
    "if not os.path.exists(summaries_dir):\n",
    "    os.makedirs(summaries_dir)\n",
    "\n",
    "# Create the CSV file and write the header\n",
    "with open(\"book_summaries.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"text\", \"summary\"])\n",
    "\n",
    "    # Process each book in the directory\n",
    "    for filename in os.listdir(books_dir):\n",
    "        book_filepath = os.path.join(books_dir, filename)\n",
    "\n",
    "        # Read the book text from the file\n",
    "        with open(book_filepath, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Summarize the text using the TextRank algorithm\n",
    "        summarizer = TextRankSummarizer()\n",
    "        tokenizer = Tokenizer(\"english\")\n",
    "        parser = PlaintextParser(text, tokenizer)\n",
    "        summary = summarizer(parser.document, sentences_count=3)\n",
    "\n",
    "        # Write the summary to a file with the same name as the original file\n",
    "        summary_filename = os.path.splitext(filename)[0] + \"_summary.txt\"\n",
    "        summary_filepath = os.path.join(summaries_dir, summary_filename)\n",
    "        with open(summary_filepath, \"w\", encoding=\"utf-8\") as summary_file:\n",
    "            summary_file.write(str(summary))\n",
    "\n",
    "        # Write the book text and summary text to the CSV file\n",
    "        writer.writerow([text, str(summary)])\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e906e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    " from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "import os\n",
    "\n",
    "# Define the path to the directory where the books are stored\n",
    "books_dir = \"summaries\"\n",
    "\n",
    "# Define the path to the directory where the summaries will be stored\n",
    "summaries_dir = \"summariess\"\n",
    "    # Process each book in the directory\n",
    "    for filename in os.listdir(books_dir):\n",
    "        book_filepath = os.path.join(books_dir, filename)\n",
    "\n",
    "        # Read the book text from the file\n",
    "        with open(book_filepath, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Summarize the text using the TextRank algorithm\n",
    "        summarizer = TextRankSummarizer()\n",
    "        tokenizer = Tokenizer(\"english\")\n",
    "        parser = PlaintextParser(text, tokenizer)\n",
    "        summary = summarizer(parser.document, sentences_count=3)\n",
    "\n",
    "        # Write the summary to a file with the same name as the original file\n",
    "        summary_filename = os.path.splitext(filename)[0] + \"_summary.txt\"\n",
    "        summary_filepath = os.path.join(summaries_dir, summary_filename)\n",
    "        with open(summary_filepath, \"w\", encoding=\"utf-8\") as summary_file:\n",
    "            summary_file.write(str(summary))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
